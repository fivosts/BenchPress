# A tiny corpus of OpenCL kernels and a correspondingly small model.
# It should take a few minutes to train on a reasonably powerful GPU.
# File: //deeplearning/deepsmith/proto/clgen.proto
# Proto: clgen.Instance
#
# Copyright (c) 2016-2020 Chris Cummins.
#
# clgen is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# clgen is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with clgen.  If not, see <https://www.gnu.org/licenses/>.
working_dir: "tiny_maskLM" ## This path is relative to "workspace_dir"
model {
  corpus {
    local_tar_archive: "$PWD/corpus.tar.bz2"
    maskLM_atomizer {
      wordpiece_tokenization: false
    }
    contentfile_separator: "\n\n"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:ClangPreprocessWithShim"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:Compile"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:NormalizeIdentifiers"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:StripDoubleUnderscorePrefixes"
    preprocessor: "deeplearning.clgen.preprocessors.common:StripDuplicateEmptyLines"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:SanitizeKernelPrototype"
    preprocessor: "deeplearning.clgen.preprocessors.common:StripTrailingWhitespace"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:ClangFormat"
    preprocessor: "deeplearning.clgen.preprocessors.common:MinimumLineCount3"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:Compile"
  }
  architecture {
    backend: TENSORFLOW_BERT
    hidden_size: 288
    num_hidden_layers: 2
    num_attention_heads: 12
    intermediate_size: 1024
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    max_position_embeddings: 128
    type_vocab_size: 16
    initializer_range: 0.02
  }
  training {
    num_train_steps: 10000
    num_warmup_steps: 4000
    sequence_length: 128
    batch_size: 64
    max_predictions_per_seq: 20
    dupe_factor: 2
    masked_lm_prob: 0.15
    random_seed: 12345
    shuffle_corpus_contentfiles_between_epochs: true
    adam_optimizer {
      initial_learning_rate_micros: 2000  # = 0.02 real value
    }
  }
}
sampler {
  start_text: "kernel void A([HOLE]}"
  batch_size: 1
  sequence_length: 64
  temperature_micros: 800000  # = 0.8 real value
  termination_criteria {
    symtok {
      depth_increase_token: "{"
      depth_decrease_token: "}"
    }
  }
  termination_criteria {
    maxlen {
      maximum_tokens_in_sample: 500
    }
  }
}
