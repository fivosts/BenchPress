

For dataset systems check here: https://stackoverflow.com/questions/53576113/most-efficient-way-to-use-a-large-data-set-for-pytorch

data collator is loaded through a dataloader. Dataloader needs dataset, batch size, sampler, and collator. (What happens with huge datasets ?). Sampler is easy to setup. You need RandomSampler, by passing only dataset.

Data collator on runtime takes an input, masks it and returns input_ids and labels. Labels summarize all masked_lm_ids, masked_lm_positions and masked_lm_weights are not needed because all other un-masked positions are set to -100.

Some equivalency in tensors:
	input_ids  	   = input_ids
	attention_mask = input_mask
	token_type_ids = segment_ids
	position_ids   = only used for positional embeddings. Defaulted to np.arange(
					 config.max_position_embeddings) but can be changed to sinusoidal etc.
	head_mask      = not sure but I thing it is used as binary array to block or allow ith attention head to participate.