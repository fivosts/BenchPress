// The file defines the protos for specificying CLgen models.
//
// Please ignore the "optional" proto syntax, ALL FIELDS MUST BE SET.
// This is to ensure consistent hashing of proto instances to a unique checksum,
// as default values are ommitted in serialized protos. Unfortunately, this
// means setting a value to any new field in all of the proto files across this
// entire repository (and any which are not tracked in this repo).
//
// Copyright (c) 2016-2020 Chris Cummins.
//
// clgen is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
//
// clgen is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with clgen.  If not, see <https://www.gnu.org/licenses/>.

syntax = "proto2";

package clgen;

option go_package = "clgenpb";
option java_multiple_files = true;
option java_outer_classname = "ModelProto";
option java_package = "com.clgen";

import "deeplearning/clgen/proto/corpus.proto";

// The specification of a CLgen model.
message Model {
  optional clgen.Corpus corpus = 1;
  optional NetworkArchitecture architecture = 2;
  optional TrainingOptions training = 3;
}

// The specification of a CLgen language model.
message NetworkArchitecture {
  enum Backend {
    TENSORFLOW_SEQ = 0;
    TENSORFLOW_BERT = 1;
    KERAS_SEQ = 2;
  }
  enum NeuronType {
    LSTM = 0;
    RNN = 1;
    GRU = 2;
  }
  optional Backend backend = 1;
  // The size of the input embedding layer. Only required if backend == KERAS_SEQ.
  // Must be > 0.
  optional int32 embedding_size = 2;
  // The type of neuron. Valid options are: {"lstm","rnn","gru"}.
  optional NeuronType neuron_type = 3;
  // The number of neurons in each layer of the network.
  optional int32 neurons_per_layer = 4;
  // The total number of layers in the network.
  optional int32 num_layers = 5;
  // If greater than zero, this adds a dropout layer after each layer of neurons
  // with probability post_alyer_drop_micros / 1000000. E.g. a value of 2000
  // would insert a dropout with probability of 0.2.
  optional int32 post_layer_dropout_micros = 6;
  //Size of the encoder layers and the pooler layer.
  optional int32 hidden_size = 7;
  // The messages below correspong to BERT parameters.
  // Number of hidden layers in the Transformer encoder.
  optional int32 num_hidden_layers = 8;
  // Number of attention heads for each attention layer in the Transformer encoder.
  optional int32 num_attention_heads = 9;
  // The size of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder.
  optional int32 intermediate_size = 10;
  // The non-linear activation function (function or string) in the encoder and pooler.
  optional string hidden_act = 11;
  // The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.
  optional float hidden_dropout_prob = 12;
  // The dropout ratio for the attention probabilities.
  optional float attention_probs_dropout_prob = 13;
  // The maximum sequence length that this model might ever be used with. 
  // Typically set this to something large just in case (e.g., 512 or 1024 or 2048).
  optional int32 max_position_embeddings = 14;
  // The vocabulary size of the `token_type_ids` passed into `BertModel`.
  optional int32 type_vocab_size = 15;
  // The stdev of the truncated_normal_initializer for initializing all weight matrices.
  optional float initializer_range = 16;
}

// Options used for training a CLgen language model.
message TrainingOptions {
  // The number of epochs to train the network for.
  optional int32 num_epochs = 1;
  // The length of training sequences.
  optional int32 num_train_steps = 2;
  // BERT only. Number of training steps.
  optional int32 num_warmup_steps = 3;
  // BERT only. Number of warmup steps.
  optional int32 sequence_length = 4;
  // Maximum number of masked LM predictions per sequence.
  optional int32 max_predictions_per_seq = 5;
  // Number of times to duplicate the input data (with different masks).
  optional int32 dupe_factor = 6;
  // Masked LM probability.
  optional float masked_lm_prob = 7;
  // Random seed for data generation.
  optional int32 random_seed = 8;
  // If true, shuffle the order of contentfiles in the corpus between each
  // training epoch.
  optional bool shuffle_corpus_contentfiles_between_epochs = 9;
  // The training batch size. Note that this is only a *requested* batch size,
  // there may be cases where the runtime decides to modify this value. For
  // example, when the corpus size is smaller than the batch size. Any changes
  // to this value at runtime will be logged as errors.
  optional int32 batch_size = 10;
  // In case of BERT model, a specific data generator is needed.
  optional DataGenerator data_generator = 11;
  // The optimizer configuration.
  oneof optimizer {
    AdamOptimizer adam_optimizer = 12;
    RmsPropOptimizer rmsprop_optimizer = 13;
  }
}

message DataGenerator {
  // Represent single training instance as whole padded kernel, 
  // or arbitrary statement sequences. Valid options are "kernel" or "statement".
  optional string datapoint_type   = 1;
  // Use [START] and [END] meta tokens at the beginning and end of each sequence.
  optional bool   use_start_end    = 2;
  // Number of steps that constitute an epoch. Checkpoints and samples are taken once every epoch.
  optional int32  steps_per_epoch  = 3;
  // Select a value between 0 and 100. This percentage will be used to split dataset into
  // training and validation. Validation set will not be seen during training.
  optional int32  validation_split = 4;
  // single token masks are BERT's default. Alternatively, use a hole token
  // to represent an arbitrary amount of hidden tokens.
  oneof mask_technique {
    Mask mask = 5;
    Hole hole = 6;
  }
  repeated ValidationOpts validation_opts = 7;
}

message Mask {
  // When selecting an index in the input tensor, the original BERT model gives 80% chance
  // to replace it with a MASK, a 10% chance to replace it with another random token
  // and another 10% to leave it be after all. Set True to enable this behavior. Otherwise,
  // when selecting an index in the input, this will be replaced by a MASK.
  optional bool random_placed_mask = 1;
}

message Hole {
  // In case sequences are hole-d, choose upper 
  // bound range of possible hole length (will be [0, hole_length]).
  optional int32 hole_length = 1;
  // Learning holes is a difficult task. Stage training to start from
  // many single-token holes (equivalent to masks) and slightly move to fewer
  // and increasingly lengthier holes.
  optional bool  stage_training = 2;
}

message ValidationOpts {
  oneof option {
    int32 replicate_different_length = 1;
    // TODO extend here for more val opts.
  }
}

// The field name suffix '_micros' shows that the value contained in the field
// is converted at runtime to a floating point number by dividing it by 1e6.
// The reason for _micros fields is so that we can realiably encode and compare
// protos without having to worry about floating point rounding and comparisons.
message AdamOptimizer {
  // The initial learning rate. Must be >= 0. A recommended starting value is
  // 2000 (i.e. real value 0.002).
  optional int32 initial_learning_rate_micros = 1;
  // The ratio by which the learning rate decays per epoch of training. Must be
  // >= 0. A recommended starting value is 5000 (i.e. real value 0.05).
  optional int32 learning_rate_decay_per_epoch_micros = 2;
  // Must be in real value range 0 < beta_1 < 1. A recommended starting value
  // is 900000 (i.e. real value 0.9).
  optional int32 beta_1_micros = 3;
  // Must be in real value range 0 < beta_2 < 1. A recommended starting value
  // is 999000 (i.e. real value 0.999).
  optional int32 beta_2_micros = 4;
  // The normalized gradient clip value. A recommended starting value is 5000000
  // (ie. real value 5.0).
  optional int32 normalized_gradient_clip_micros = 5;
}

message RmsPropOptimizer {
  // The initial learning rate. Must be >= 0. A recommended starting value is
  // 1000 (i.e. real value 0.001).
  optional int32 initial_learning_rate_micros = 1;
  // The ratio by which the learning rate decays per epoch of training. Must be
  // >= 0. A recommended starting value is 0.
  optional int32 learning_rate_decay_per_epoch_micros = 2;
}

// A generated sample. Instances of this proto are returned by a Model's
// Sample() method.
message Sample {
  optional string sample_feed   = 1;
  optional string text          = 2;
  optional string encoded_text  = 3;
  optional int32 sample_time_ms = 4;
  // Sampling may be batches, so that the sum of sample_time_ms over a range
  // of samples may be much higher than the actual amount of time required to
  // sample the set. This field contains the number of milliseconds between the
  // last sample completing and this sample completing, so that by summing
  // wall_time_ms, it is possible to get an accurate idea of the actual time
  // taken to produce a set of samples.
  optional int32 wall_time_ms              = 5;
  optional int64 sample_start_epoch_ms_utc = 6;
  optional int32 num_tokens                = 7;
  optional bool  categorical_sampling      = 8;
  optional int32 train_step                = 9;
  optional string date_added               = 10;
}
