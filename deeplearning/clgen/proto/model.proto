// The file defines the protos for specificying CLgen models.
//
// Please ignore the "optional" proto syntax, ALL FIELDS MUST BE SET.
// This is to ensure consistent hashing of proto instances to a unique checksum,
// as default values are ommitted in serialized protos. Unfortunately, this
// means setting a value to any new field in all of the proto files across this
// entire repository (and any which are not tracked in this repo).
//
// Copyright (c) 2016-2020 Chris Cummins.
//
// clgen is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
//
// clgen is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with clgen.  If not, see <https://www.gnu.org/licenses/>.

syntax = "proto2";

package clgen;

option go_package = "clgenpb";
option java_multiple_files = true;
option java_outer_classname = "ModelProto";
option java_package = "com.clgen";

import "deeplearning/clgen/proto/corpus.proto";

// The specification of a CLgen model.
message Model {
  optional clgen.PreTrainCorpus pre_train_corpus = 1;
  optional clgen.Corpus         corpus           = 2;
  optional NetworkArchitecture  architecture     = 3;
  optional TrainingOptions      training         = 4;
}

// The specification of a CLgen language model.
message NetworkArchitecture {
  enum Backend {
    TENSORFLOW_SEQ  = 0;
    TENSORFLOW_BERT = 1;
    TORCH_BERT      = 2;
    KERAS_SEQ       = 3;
    INCODER_1B      = 4;
    INCODER_6B      = 5;
  }
  enum NeuronType {
    LSTM = 0;
    RNN  = 1;
    GRU  = 2;
  }
  optional Backend backend = 1;
  // The size of the input embedding layer. Only required if backend == KERAS_SEQ.
  // Must be > 0.
  optional int32 embedding_size = 2;
  // The type of neuron. Valid options are: {"lstm","rnn","gru"}.
  optional NeuronType neuron_type = 3;
  // The number of neurons in each layer of the network.
  optional int32 neurons_per_layer = 4;
  // The total number of layers in the network.
  optional int32 num_layers = 5;
  // If greater than zero, this adds a dropout layer after each layer of neurons
  // with probability post_alyer_drop_micros / 1000000. E.g. a value of 2000
  // would insert a dropout with probability of 0.2.
  optional int32 post_layer_dropout_micros = 6;
  //Size of the encoder layers and the pooler layer.
  optional int32 hidden_size = 7;
  // The messages below correspong to BERT parameters.
  // Number of hidden layers in the Transformer encoder.
  optional int32 num_hidden_layers = 8;
  // Number of attention heads for each attention layer in the Transformer encoder.
  optional int32 num_attention_heads = 9;
  // The size of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder.
  optional int32 intermediate_size = 10;
  // The non-linear activation function (function or string) in the encoder and pooler.
  optional string hidden_act = 11;
  // The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.
  optional float hidden_dropout_prob = 12;
  // The dropout ratio for the attention probabilities.
  optional float attention_probs_dropout_prob = 13;
  // The maximum sequence length that this model might ever be used with. 
  // Typically set this to something large just in case (e.g., 512 or 1024 or 2048).
  optional int32 max_position_embeddings = 14;
  // The vocabulary size of the `token_type_ids` passed into `BertModel`.
  optional int32 type_vocab_size = 15;
  // The stdev of the truncated_normal_initializer for initializing all weight matrices.
  optional float initializer_range = 16;
  // The epsilon used by the layer normalization layers.
  optional float layer_norm_eps = 17;
  // Usage flag for feature vector encoding during training.
  optional bool feature_encoder = 18;
  // Concatenated raw features vector input length.
  optional int32 feature_sequence_length = 19;
  // Embedding dimension of feature transformer input.
  optional int32 feature_embedding_size = 20;
  // Dropout probs of feature transformer encoder.
  optional float feature_dropout_prob = 21;
  // Threshold up to which every numerical value maps to a single token.
  optional int32 feature_singular_token_thr = 22;
  // Maximum value allowed for feature numerical encoding.
  optional int32 feature_max_value_token = 23;
  // Length of value range mapping to single token for feature tokenizer.
  optional int32 feature_token_range = 24;
  // Num of attention heads for feature transformer
  optional int32 feature_num_attention_heads = 25;
  // Size of feature encoder's internal feedforward.
  optional int32 feature_transformer_feedforward = 26;
  // Layer norm epsilon for feature encoder.
  optional float feature_layer_norm_eps = 27;
  // Num of hidden layers for feature encoder.
  optional int32 feature_num_hidden_layers = 28;
}

// Options used for training a CLgen language model.
message TrainingOptions {
  // The number of epochs to train the network for.
  optional int32 num_epochs = 1;
  // The length of training sequences.
  optional int32 num_train_steps = 2;
  // BERT only. Number of training steps.
  optional int32 num_pretrain_steps = 3;
  // BERT only. Number of pre-training steps.
  optional int32 num_warmup_steps = 4;
  optional int32 num_prewarmup_steps = 5;
  // BERT only. Number of warmup steps.
  optional int32 sequence_length = 6;
  // Maximum number of masked LM predictions per sequence.
  optional int32 max_predictions_per_seq = 7;
  // Number of times to duplicate the input data (with different masks).
  optional int32 dupe_factor = 8;
  // Masked LM probability.
  optional float masked_lm_prob = 9;
  // Random seed for data generation.
  optional int32 random_seed = 10;
  // If true, shuffle the order of contentfiles in the corpus between each
  // training epoch.
  optional bool shuffle_corpus_contentfiles_between_epochs = 11;
  // The training batch size. Note that this is only a *requested* batch size,
  // there may be cases where the runtime decides to modify this value. For
  // example, when the corpus size is smaller than the batch size. Any changes
  // to this value at runtime will be logged as errors.
  optional int32 batch_size = 12;
  // In case of BERT model, a specific data generator is needed.
  optional DataGenerator data_generator = 13;
  // The optimizer configuration.
  oneof optimizer {
    AdamOptimizer adam_optimizer = 14;
    RmsPropOptimizer rmsprop_optimizer = 15;
  }
}

message DataGenerator {
  // Represent single training instance as whole padded kernel, 
  // or arbitrary statement sequences. Valid options are "kernel" or "statement".
  optional string datapoint_type         = 1;
  // When datapoints should be pre-processed for training/validation/sampling.
  // 'pre': Raw corpus is masked and stored. Then used.
  // 'online': Raw corpus is stored. During training/validation/sampling,
  // datapoints are pre-processed on the fly and provided to the model.
  optional string datapoint_time         = 2;
  // Use [START] and [END] meta tokens at the beginning and end of each sequence.
  optional bool   use_start_end          = 3;
  // If datapoints are 'kernel', kernels > seq len are discarded. If true, they are kept and truncated instead.
  optional bool   truncate_large_kernels = 4;
  // Number of steps that constitute an epoch. Checkpoints and samples are taken once every epoch.
  optional int32  steps_per_epoch        = 5;
  // Select a value between 0 and 100. This percentage will be used to split dataset into
  // training and validation. Validation set will not be seen during training.
  optional int32  validation_split       = 6;
  // single token masks are BERT's default. Alternatively, use a hole token
  // to represent an arbitrary amount of hidden tokens.
  oneof mask_technique {
    Mask mask = 7;
    Hole hole = 8;
    MaskSeq mask_seq = 9;
  }
  repeated ValidationSet validation_set = 10;
}

message Mask {
  // When selecting an index in the input tensor, the original BERT model gives 80% chance
  // to replace it with a MASK, a 10% chance to replace it with another random token
  // and another 10% to leave it be after all. Set True to enable this behavior. Otherwise,
  // when selecting an index in the input, this will be replaced by a MASK.
  optional bool random_placed_mask = 1;
}

message Hole {
  // In case sequences are hole-d, choose upper 
  // bound range of possible hole length (will be [0, hole_length]).
  oneof length {
    int32 absolute_length = 1;
    float relative_length = 2;
  }
  // Select distribution from which each hole length will be sampled.
  oneof length_distribution {
    bool    uniform_distribution = 3;
    Normal  normal_distribution  = 4;
  }
  // Learning holes is a difficult task. Stage training to start from
  // many single-token holes (equivalent to masks) and slightly move to fewer
  // and increasingly lengthier holes.
  optional bool stage_training = 5;
}

message MaskSeq {
  // Special category of a hole represented as a set of masks.
  oneof length {
    int32 absolute_length = 1;
    float relative_length = 2;
  }
  // Select distribution from which each hole length will be sampled.
  oneof length_distribution {
    bool    uniform_distribution = 3;
    Normal  normal_distribution  = 4;
  }
  // Learning holes is a difficult task. Stage training to start from
  // many single-token holes (equivalent to masks) and slightly move to fewer
  // and increasingly lengthier holes.
  optional bool stage_training = 5;
}

message Normal {
  optional float mean     = 1;
  optional float variance = 2;
}

message ValidationSet {
  optional int32 max_predictions_per_seq = 1;
  oneof option {
    // Create new sets out of the original corpus,
    // to use them for validation or sampling.
    // Useful to test the model against different specs.
    Mask mask = 3;
    Hole hole = 4;
  }
}

// The field name suffix '_micros' shows that the value contained in the field
// is converted at runtime to a floating point number by dividing it by 1e6.
// The reason for _micros fields is so that we can realiably encode and compare
// protos without having to worry about floating point rounding and comparisons.
message AdamOptimizer {
  // The initial learning rate. Must be >= 0. A recommended starting value is
  // 2000 (i.e. real value 0.002).
  optional int32 initial_learning_rate_micros = 1;
  // The ratio by which the learning rate decays per epoch of training. Must be
  // >= 0. A recommended starting value is 5000 (i.e. real value 0.05).
  optional int32 learning_rate_decay_per_epoch_micros = 2;
  // Must be in real value range 0 < beta_1 < 1. A recommended starting value
  // is 900000 (i.e. real value 0.9).
  optional int32 beta_1_micros = 3;
  // Must be in real value range 0 < beta_2 < 1. A recommended starting value
  // is 999000 (i.e. real value 0.999).
  optional int32 beta_2_micros = 4;
  // The normalized gradient clip value. A recommended starting value is 5000000
  // (ie. real value 5.0).
  optional int32 normalized_gradient_clip_micros = 5;
}

message RmsPropOptimizer {
  // The initial learning rate. Must be >= 0. A recommended starting value is
  // 1000 (i.e. real value 0.001).
  optional int32 initial_learning_rate_micros = 1;
  // The ratio by which the learning rate decays per epoch of training. Must be
  // >= 0. A recommended starting value is 0.
  optional int32 learning_rate_decay_per_epoch_micros = 2;
}

// A generated sample. Instances of this proto are returned by a Model's
// Sample() method.
message Sample {
  optional string original_input         = 1;
  optional string sample_feed            = 2;
  optional string text                   = 3;
  optional string encoded_text           = 4;
  optional int32  sample_time_ms         = 5;
  optional string sample_indices         = 6;
  optional string encoded_sample_indices = 7;
  optional string feature_vector         = 8;
  // Sampling may be batches, so that the sum of sample_time_ms over a range
  // of samples may be much higher than the actual amount of time required to
  // sample the set. This field contains the number of milliseconds between the
  // last sample completing and this sample completing, so that by summing
  // wall_time_ms, it is possible to get an accurate idea of the actual time
  // taken to produce a set of samples.
  optional int32 wall_time_ms              = 9;
  optional int64 sample_start_epoch_ms_utc = 10;
  optional int32 num_tokens                = 11;
  optional bool  compile_status            = 12;
  optional bool  categorical_sampling      = 13;
  optional int32 train_step                = 14;
  optional string date_added               = 15;
}
