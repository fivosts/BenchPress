// This files defines the specification of the reinforcement learning model.
syntax = "proto2";

package clgen;

option go_package = "clgenpb";
option java_multiple_files = true;
option java_outer_classname = "ReinforcementLearningProto";
option java_package = "com.clgen";

import "deeplearning/clgen/proto/model.proto";

message RLModel {
  oneof target_features {
    bool train_set = 1;
    bool random    = 2;
  }
  optional RLAgent agent          = 3;
  optional Model   language_model = 4;
}

message RLAgent {
  optional FeatureTokenizer feature_tokenizer = 1;
  optional ActionQV         action_qv         = 2;
  optional ActionLM         action_lm         = 3;
}

message FeatureTokenizer {
  optional int32 feature_max_value_token    = 1;
  optional int32 feature_singular_token_thr = 2;
  optional int32 feature_token_range        = 3;
}

message ActionQV {
  // Transformer Encoder's length for feature vectors.
  optional int32 feature_sequence_length      = 1;
  // Transformer Decoder's sequence length for source code.
  optional int32 max_position_embeddings      = 2;
  // Hidden size of Transformers.
  optional int32 hidden_size                  = 3;
  // # Attention Heads for transformers.
  optional int32 num_attention_heads          = 4;
  // Intermediate size for transformers.
  optional int32 intermediate_size            = 5;
  // Number of transformer hidden layers.
  optional int32 num_hidden_layers            = 6;
  // Layer normalization epsilon.
  optional float layer_norm_eps               = 7;
  // Dropout probability.
  optional float hidden_dropout_prob          = 8;
  // Hidden act for action prediction head.
  optional string hidden_act                  = 9;
  // Dropout probs of attention_heads
  optional float attention_probs_dropout_prob = 10;
  // type_vocab size
  optional int32 type_vocab_size              = 11;
  // initializer_range
  optional float initializer_range            = 12;
}

message ActionLM {
  // Transformer Encoder's length for feature vectors.
  optional int32 feature_sequence_length      = 1;
  // Transformer Decoder's sequence length for source code.
  optional int32 max_position_embeddings      = 2;
  // Hidden size of Transformers.
  optional int32 hidden_size                  = 3;
  // # Attention Heads for transformers.
  optional int32 num_attention_heads          = 4;
  // Intermediate size for transformers.
  optional int32 intermediate_size            = 5;
  // Number of transformer hidden layers.
  optional int32 num_hidden_layers            = 6;
  // Layer normalization epsilon.
  optional float layer_norm_eps               = 7;
  // Dropout probability.
  optional float hidden_dropout_prob          = 8;
  // Hidden act for action prediction head.
  optional string hidden_act                  = 9;
  // Dropout probs of attention_heads
  optional float attention_probs_dropout_prob = 10;
  // type_vocab size
  optional int32 type_vocab_size              = 11;
  // initializer_range
  optional float initializer_range            = 12;
}
