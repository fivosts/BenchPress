// This files defines the specification of the reinforcement learning model.
syntax = "proto2";

package clgen;

option go_package = "clgenpb";
option java_multiple_files = true;
option java_outer_classname = "ReinforcementLearningProto";
option java_package = "com.clgen";

import "deeplearning/clgen/proto/model.proto";

message RLModel {
  oneof target_features {
    bool train_set = 1;
    bool random    = 2;
  }
  optional RLAgent agent          = 3;
  optional Model   language_model = 4;
}

message RLAgent {
  optional FeatureTokenizer feature_tokenizer = 1;
  optional ActionQV         action_qv         = 2;
  optional ActionLM         action_lm         = 3;
  optional int32            batch_size        = 4;
}

message FeatureTokenizer {
  // Max value that can be tokenized. Anything above that is UNK.
  optional int32 feature_max_value_token    = 1;
  // Threshold of values being singularly tokenized (not represented as range).
  optional int32 feature_singular_token_thr = 2;
  // Range width of tokenized values after singularity threshold
  optional int32 feature_token_range        = 3;
  // Sequence length of tokenized feature vectors.
  optional int32 feature_sequence_length    = 4;
}

message ActionQV {
  // Hidden size of Transformers.
  optional int32 hidden_size                     = 1;
  // # Attention Heads for transformers.
  optional int32 num_attention_heads             = 2;
  // Intermediate size for transformers.
  optional int32 intermediate_size               = 3;
  // Number of transformer hidden layers.
  optional int32 num_hidden_layers               = 4;
  // Layer normalization epsilon.
  optional float layer_norm_eps                  = 5;
  // Dropout probability.
  optional float hidden_dropout_prob             = 6;
  // Hidden act for action prediction head.
  optional string hidden_act                     = 7;
  // Dropout probs of attention_heads
  optional float attention_probs_dropout_prob    = 8;
  // type_vocab size
  optional int32 type_vocab_size                 = 9;
  // initializer_range
  optional float initializer_range               = 10;
  // Sampling temperature for action type head
  optional int32 action_type_temperature_micros  = 11;
  // Sampling temperature for action index head
  optional int32 action_index_temperature_micros = 12;
}

message ActionLM {
  // Hidden size of Transformers.
  optional int32 hidden_size                  = 1;
  // # Attention Heads for transformers.
  optional int32 num_attention_heads          = 2;
  // Intermediate size for transformers.
  optional int32 intermediate_size            = 3;
  // Number of transformer hidden layers.
  optional int32 num_hidden_layers            = 4;
  // Layer normalization epsilon.
  optional float layer_norm_eps               = 5;
  // Dropout probability.
  optional float hidden_dropout_prob          = 6;
  // Hidden act for action prediction head.
  optional string hidden_act                  = 7;
  // Dropout probs of attention_heads
  optional float attention_probs_dropout_prob = 8;
  // type_vocab size
  optional int32 type_vocab_size              = 9;
  // initializer_range
  optional float initializer_range            = 10;
  // Sampling temperature for token LM.
  optional float token_temperature_micros     = 11;
}
