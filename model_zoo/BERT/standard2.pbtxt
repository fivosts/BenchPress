# Compared to standard:
# 1. 4 hidden layers vs 2
# 2. warmup is 20,000 vs 5,000
# 3. dupe factor is 40 instead of 50
# 5. Holes can be up to 15.
# 6. Learning rate increased to 45 from 40.
working_dir: "BERT" ## This path is relative to "workspace_dir", which is an app FLAG
model {
  corpus {
    local_tar_archive: "$PWD/../corpus/ultimate_corpus.tar.bz2"
    atomizer {
      token_type: "word"
      token_list: "deeplearning/clgen/corpuses/token_lists.json"
      mask_tokens: true
      wordpiece_tokenization: false
    }
    contentfile_separator: "\n\n"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:ClangPreprocessWithShim"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:Compile"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:NormalizeIdentifiers"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:StripDoubleUnderscorePrefixes"
      preprocessor: "deeplearning.clgen.preprocessors.common:StripDuplicateEmptyLines"
      preprocessor: "deeplearning.clgen.preprocessors.common:StripMultipleWhitespaces"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:SanitizeKernelPrototype"
      preprocessor: "deeplearning.clgen.preprocessors.common:StripTrailingWhitespace"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:ClangFormat"
      preprocessor: "deeplearning.clgen.preprocessors.common:MinimumLineCount3"
      preprocessor: "deeplearning.clgen.preprocessors.common:ExtractOnlySingleKernels"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:Compile"
  }
  architecture {
    backend: TORCH_BERT
    hidden_size: 1024
    num_hidden_layers: 4
    num_attention_heads: 4
    intermediate_size: 1024
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    max_position_embeddings: 1024
    layer_norm_eps: 1e-12
    type_vocab_size: 16
    initializer_range: 0.02
  }
  training {
    num_train_steps: 10000000
    num_warmup_steps: 20000
    sequence_length: 1024
    batch_size: 32
    max_predictions_per_seq: 1
    dupe_factor: 40
    masked_lm_prob: 0.9
    random_seed: 12345
    shuffle_corpus_contentfiles_between_epochs: true
    data_generator {
      datapoint_type: "kernel"
      use_start_end : true
      steps_per_epoch: 50000
      validation_split: 10
      hole {
        hole_length: 15
        uniform_distribution: true
		    stage_training: false
      }
    }
    adam_optimizer {
      initial_learning_rate_micros: 45  # = 0.02 real value
    }
  }
}
sampler {
  sample_corpus {
    corpus_config {
      sampling_type: "active"
      max_predictions_per_seq: 1
      masked_lm_prob: 0.9
      hole {
        hole_length: 15
        uniform_distribution: true
      }
    }
    corpus {
      local_tar_archive: "$PWD/model_zoo/corpus/ultimate_corpus.tar.bz2"
      atomizer {
        token_type: "word"
        token_list: "deeplearning/clgen/corpuses/token_lists.json"
        mask_tokens: true
        wordpiece_tokenization: false
      }
      contentfile_separator: "\n\n"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:ClangPreprocessWithShim"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:Compile"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:NormalizeIdentifiers"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:StripDoubleUnderscorePrefixes"
      preprocessor: "deeplearning.clgen.preprocessors.common:StripDuplicateEmptyLines"
      preprocessor: "deeplearning.clgen.preprocessors.common:StripMultipleWhitespaces"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:SanitizeKernelPrototype"
      preprocessor: "deeplearning.clgen.preprocessors.common:StripTrailingWhitespace"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:ClangFormat"
      preprocessor: "deeplearning.clgen.preprocessors.common:MinimumLineCount3"
      preprocessor: "deeplearning.clgen.preprocessors.common:ExtractOnlySingleKernels"
      preprocessor: "deeplearning.clgen.preprocessors.opencl:Compile" 
    }
  }
  batch_size: 4
  sequence_length: 1024
  temperature_micros: 700000  # = 0.8 real value
  termination_criteria {
    maxlen {
      maximum_tokens_in_sample: 1024
    }
  }
}
