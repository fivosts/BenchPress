# Compared to standard:
# 1. 12 attention heads vs 4
# 2. Hidden size is 768 vs 1024
# 3. Intermediate size is 768 vs 1024
# 4. warmup is 10,000 vs 5,000
# 5. dupe factor is 20 instead of 50
# 6. There can be up to 3 holes.
# 7. Holes can be up to 10 tokens long.
# 8. Learning rate increased to 45 from 40.
working_dir: "AST" ## This path is relative to "workspace_dir", which is an app FLAG
language_model {
  corpus {
    local_tar_archive: "$PWD/../corpus/corpus_250.tar.bz2"
    tokenizer {
      token_type: "incoder-1b"
    }
    contentfile_separator: "\n\n\n\n"
    preprocessor: "deeplearning.benchpress.preprocessors.opencl:ClangPreprocessWithShim"
    preprocessor: "deeplearning.benchpress.preprocessors.opencl:StripDoubleUnderscorePrefixes"
    preprocessor: "deeplearning.benchpress.preprocessors.opencl:ClangFormat"
    preprocessor: "deeplearning.benchpress.preprocessors.opencl:InvertKernelSpecifier"
    preprocessor: "deeplearning.benchpress.preprocessors.opencl:ExtractOnlySingleKernels"
    preprocessor: "deeplearning.benchpress.preprocessors.opencl:StringKernelsToSource"
    preprocessor: "deeplearning.benchpress.preprocessors.opencl:Compile"
    preprocessor: "deeplearning.benchpress.preprocessors.opencl:NormalizeIdentifiers"
    preprocessor: "deeplearning.benchpress.preprocessors.common:StripDuplicateEmptyLines"
    preprocessor: "deeplearning.benchpress.preprocessors.common:StripMultipleWhitespaces"
    preprocessor: "deeplearning.benchpress.preprocessors.opencl:SanitizeKernelPrototype"
    preprocessor: "deeplearning.benchpress.preprocessors.common:StripTrailingWhitespace"
    preprocessor: "deeplearning.benchpress.preprocessors.opencl:ClangFormat"
    preprocessor: "deeplearning.benchpress.preprocessors.common:MinimumLineCount3"
    preprocessor: "deeplearning.benchpress.preprocessors.opencl:Compile"
  }
  architecture {
    backend: INCODER_1B
  }
  training {
    num_train_steps: 10000000
    num_warmup_steps: 20000
    sequence_length: 2048
    batch_size: 2
    max_predictions_per_seq: 1
    dupe_factor: 1
    masked_lm_prob: 0.4
    random_seed: 12345
    shuffle_corpus_contentfiles_between_epochs: true
    data_generator {
      datapoint_type: "kernel"
      datapoint_time: "online"
      use_start_end : false
      truncate_large_kernels: true
      steps_per_epoch: 200000
      validation_split: 0
      hole {
        relative_length: 0.9
        uniform_distribution: true
		    stage_training: false
      }
    }
    adam_optimizer {
      initial_learning_rate_micros: 45  # = 0.02 real value
    }
  }
}
# sampler {
#   start_text: "kernel void A<|mask:0|>"
#   batch_size: 1
#   sequence_length: 2048
#   temperature_micros: 700000  # = 0.8 real value
#   termination_criteria {
#     maxlen {
#       maximum_tokens_in_sample: 2048
#     }
#   }
# }
sampler {
  sample_corpus {
    corpus_config {
      active {
        target: "rodinia"
        active_search_depth: 40
        active_search_width: 32
        active_dropout_prob: 0.1
        batch_size_per_feed: 2
        feature_space: "GreweFeatures"
      }
      max_predictions_per_seq: 1
      masked_lm_prob: 0.6
      hole {
        relative_length: 0.7
        uniform_distribution: true
      }
    }
    start_text: "kernel void A<|mask:0|>"
  }
  batch_size: 16
  sequence_length: 1900
  temperature_micros: 1000000  # = 0.8 real value
  termination_criteria {
    maxlen {
      maximum_tokens_in_sample: 1900
    }
  }
}
